{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ILUES_surrogate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM+zhF96TLiDIIXP0ts1Wrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZitongZhou/react_inverse/blob/master/ILUES_surrogate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU28D_oaaEPH",
        "outputId": "c8cd38f5-999c-426b-ff0a-7bf0b1bd9d24"
      },
      "source": [
        "!pip install attrdict\n",
        "!pip install --upgrade matplotlib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from attrdict) (1.15.0)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZEtkuxyZZ0d",
        "outputId": "91854d59-ae40-4278-f982-6ad7f8c179b2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle as pk\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import scipy.io\n",
        "from attrdict import AttrDict\n",
        "import argparse\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as utils\n",
        "from torch.utils.data import sampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Gamma\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import matplotlib.colors\n",
        "import matplotlib.backends.backend_pdf\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8peiJvVax5d"
      },
      "source": [
        "# DenseED, Bayesian DenseED, and CAAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuW1cEVGbeFJ"
      },
      "source": [
        "\"\"\"\n",
        "Convolutional Dense Encoder-Decoder Networks\n",
        "\n",
        "Reference:\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n",
        "\n",
        "Yinhao Zhu\n",
        "Dec 21, 2017\n",
        "Dec 30, 2017\n",
        "Jan 03, 2018\n",
        "\n",
        "Shaoxing Mo\n",
        "May 07, 2019\n",
        "\n",
        "Zitong Zhou\n",
        "Feb 18, 2021\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    # bottleneck layer, bn_size: bottleneck size\n",
        "    def __init__(self, in_features, growth_rate, drop_rate=0, bn_size=4,\n",
        "                 bottleneck=False):\n",
        "        # detect if the input features are more than bn_size x k,\n",
        "        # if yes, use bottleneck -- not much memory gain, but lose one relu\n",
        "        # I disabled the bottleneck for current implementation\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        if bottleneck and in_features > bn_size * growth_rate:\n",
        "            self.add_module('norm1', nn.BatchNorm3d(in_features))\n",
        "            self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "            self.add_module('conv1', nn.Conv3d(in_features, bn_size *\n",
        "                            growth_rate, kernel_size=1, stride=1, bias=False))\n",
        "            self.add_module('norm2', nn.BatchNorm3d(bn_size * growth_rate))\n",
        "            self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "            self.add_module('conv2', nn.Conv3d(bn_size * growth_rate, growth_rate,\n",
        "                            kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        else:\n",
        "            self.add_module('norm1', nn.BatchNorm3d(in_features))\n",
        "            self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "            self.add_module('conv1', nn.Conv3d(in_features, growth_rate,\n",
        "                            kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            y = F.dropout3d(y, p=self.drop_rate, training=self.training)\n",
        "        z = torch.cat([x, y], 1)\n",
        "        return z\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, in_features, growth_rate, drop_rate,\n",
        "                 bn_size=4, bottleneck=False):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(in_features + i * growth_rate, growth_rate,\n",
        "                                drop_rate=drop_rate, bn_size=bn_size,\n",
        "                                bottleneck=bottleneck)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, in_features, out_features, encoding=True, drop_rate=0.,\n",
        "                 last=False, out_channels=3, outsize_even=True):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm3d(in_features))\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True))\n",
        "        if encoding:\n",
        "            # reduce feature maps; half image size (input feature size is even)\n",
        "            # bottleneck impl, save memory, add nonlinearity\n",
        "            self.add_module('conv1', nn.Conv3d(in_features, out_features,\n",
        "                                              kernel_size=1, stride=1,\n",
        "                                              padding=0, bias=False))\n",
        "            if drop_rate > 0:\n",
        "                self.add_module('dropout1', nn.Dropout3d(p=drop_rate))\n",
        "            self.add_module('norm2', nn.BatchNorm3d(out_features))\n",
        "            self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "            self.add_module('conv2', nn.Conv3d(out_features, out_features,\n",
        "                                              kernel_size=3, stride=2,\n",
        "                                              padding=1, bias=False))\n",
        "            if drop_rate > 0:\n",
        "                self.add_module('dropout2', nn.Dropout3d(p=drop_rate))\n",
        "        else:\n",
        "            # decoding, transition up\n",
        "            if last:\n",
        "                ks = 6 if outsize_even else 5\n",
        "                out_convt = nn.ConvTranspose3d(out_features, out_channels,\n",
        "                                kernel_size=[2,ks,ks], stride=2, padding=[0,2,2],\n",
        "                                output_padding=[0,0,0], bias=False)\n",
        "            else:\n",
        "                out_convt = nn.ConvTranspose3d(\n",
        "                    out_features, out_features, kernel_size=3, stride=2,\n",
        "                    padding=1, output_padding=0, bias=False)\n",
        "\n",
        "            # bottleneck impl, save memory, add nonlinearity\n",
        "            self.add_module('conv1', nn.Conv3d(in_features, out_features,\n",
        "                                              kernel_size=1, stride=1,\n",
        "                                              padding=0, bias=False))\n",
        "            if drop_rate > 0:\n",
        "                self.add_module('dropout1', nn.Dropout3d(p=drop_rate))\n",
        "\n",
        "            self.add_module('norm2', nn.BatchNorm3d(out_features))\n",
        "            self.add_module('relu2', nn.ReLU(inplace=True))\n",
        "            self.add_module('convT2', out_convt)\n",
        "            if drop_rate > 0:\n",
        "                self.add_module('dropout2', nn.Dropout3d(p=drop_rate))\n",
        "\n",
        "class DenseED(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, blocks, growth_rate=16,\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, outsize_even=False,\n",
        "                 bottleneck=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_channels (int): number of input channels\n",
        "            out_channels (int): number of output channels\n",
        "            blocks: list (of odd size) of integers\n",
        "            growth_rate (int): K\n",
        "            num_init_features (int): the number of feature maps after the first\n",
        "                conv layer\n",
        "            bn_size: bottleneck size for number of feature maps (not useful...)\n",
        "            bottleneck (bool): use bottleneck for dense block or not\n",
        "            drop_rate (float): dropout rate\n",
        "            outsize_even (bool): if the output size is even or odd (e.g.\n",
        "                65 x 65 is odd, 64 x 64 is even)\n",
        "\n",
        "        \"\"\"\n",
        "        super(DenseED, self).__init__()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        if len(blocks) > 1 and len(blocks) % 2 == 0:\n",
        "            ValueError('length of blocks must be an odd number, but got {}'\n",
        "                       .format(len(blocks)))\n",
        "        enc_block_layers = blocks[: len(blocks) // 2]\n",
        "        dec_block_layers = blocks[len(blocks) // 2:]\n",
        "        self.features = nn.Sequential()\n",
        "        # First convolution ================\n",
        "        # only conv, half image size\n",
        "        self.features.add_module('in_conv',\n",
        "                    nn.Conv3d(in_channels, num_init_features,\n",
        "                            kernel_size=[3,7,7], stride=2, padding=[1,3,3], bias=False))\n",
        "\n",
        "        # Encoding / transition down ================\n",
        "        # dense block --> encoding --> dense block --> encoding\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(enc_block_layers):\n",
        "            block = _DenseBlock(num_layers=num_layers,\n",
        "                                in_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate,\n",
        "                                drop_rate=drop_rate, bottleneck=bottleneck)\n",
        "            self.features.add_module('encblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "\n",
        "            trans = _Transition(in_features=num_features,\n",
        "                                out_features=num_features // 2,\n",
        "                                encoding=True, drop_rate=drop_rate)\n",
        "            self.features.add_module('down%d' % (i + 1), trans)\n",
        "            num_features = num_features // 2\n",
        "\n",
        "        # Decoding / transition up ==============\n",
        "        # dense block --> decoding --> dense block --> decoding --> dense block\n",
        "        # if len(dec_block_layers) - len(enc_block_layers) == 1:\n",
        "        for i, num_layers in enumerate(dec_block_layers):\n",
        "            block = _DenseBlock(num_layers=num_layers,\n",
        "                                in_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate,\n",
        "                                drop_rate=drop_rate, bottleneck=bottleneck)\n",
        "            self.features.add_module('decblock%d' % (i + 1), block)\n",
        "            num_features += num_layers * growth_rate\n",
        "\n",
        "            # if this is the last decoding layer is the output layer\n",
        "            last_layer = True if i == len(dec_block_layers) - 1 else False\n",
        "\n",
        "            trans = _Transition(in_features=num_features,\n",
        "                                out_features=num_features // 2,\n",
        "                                encoding=False, drop_rate=drop_rate,\n",
        "                                last=last_layer, out_channels=out_channels,\n",
        "                                outsize_even=outsize_even)\n",
        "            self.features.add_module('up%d' % (i + 1), trans)\n",
        "            num_features = num_features // 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.features(x)\n",
        "\n",
        "        #set the source point conc to be constant if there's release\n",
        "        nonzero_source = torch.nonzero(x[:,2,:,:,:])\n",
        "        # print('nonzero ',nonzero_source)\n",
        "        if len(nonzero_source) > 0:\n",
        "            for non in nonzero_source:\n",
        "                y[non[0],0,non[1],non[2],non[3]] = x[non[0],2,non[1],non[2],non[3]]\n",
        "                \n",
        "        # use the softplus activation for concentration and head\n",
        "        y = F.softplus(y.clone(), beta=5)\n",
        "\n",
        "        # CAUTION: if the last channel is pressure, \n",
        "        # use the sigmoid activation for pressure\n",
        "        # y[:,self.out_channels-1] = torch.sigmoid(y[:,self.out_channels-1])\n",
        "\n",
        "        return y\n",
        "\n",
        "    def _num_parameters_convlayers(self):\n",
        "        n_params, n_conv_layers = 0, 0\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'conv' in name:\n",
        "                n_conv_layers += 1\n",
        "            n_params += param.numel()\n",
        "        return n_params, n_conv_layers\n",
        "\n",
        "    def _count_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            print(name)\n",
        "            print(param.size())\n",
        "            print(param.numel())\n",
        "            n_params += param.numel()\n",
        "            print('num of parameters so far: {}'.format(n_params))\n",
        "\n",
        "    def reset_parameters(self, verbose=False):\n",
        "        for module in self.modules():\n",
        "            # pass self, otherwise infinite loop\n",
        "            if isinstance(module, self.__class__):\n",
        "                continue\n",
        "            if 'reset_parameters' in dir(module):\n",
        "                if callable(module.reset_parameters):\n",
        "                    module.reset_parameters()\n",
        "                    if verbose:\n",
        "                        print(\"Reset parameters in {}\".format(module))\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AThUrQ_HZ45L"
      },
      "source": [
        "\"\"\"\n",
        "Particle approximations for posterior of Bayesian neural net used in SVGD.\n",
        "\n",
        "References:\n",
        "    Liu, Qiang, and Dilin Wang. \"Stein variational gradient descent:\n",
        "    A general purpose bayesian inference algorithm.\" NIPS. 2016.\n",
        "\n",
        "methods:\n",
        "    __init__\n",
        "    forward\n",
        "    compute_loss\n",
        "    compute_mse_nlp\n",
        "    predict\n",
        "    propagate\n",
        "\n",
        "Note: \n",
        "`torch.distributions` is not much used in this implementation to keep simple.\n",
        "Also we trade computing for memory by using for-loop rather than in a batch way.\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class BayesNN(nn.Module):\n",
        "    \"\"\"Class for Bayesian NNs with Stein Variational Gradient Descent.\n",
        "    Not for usage independently.\n",
        "    \n",
        "    Bayesian NNs: y = f(x, w) + n\n",
        "\n",
        "    uncertain weights:\n",
        "            w_i ~ Normal(w_i | mu=0, 1 / alpha) \n",
        "            alpha ~ Gamma(alpha | shape=1, rate=0.05) (shared)\n",
        "            --> w_i ~ StudentT(w_i | mu=0, lambda=shape/rate, nu=2*shape)\n",
        "            Parameterization of StudentT in Bishop p.103 Eq. (2.159)\n",
        "\n",
        "    Assumptions on noise:\n",
        "        Additive, Gaussian, homoscedastic (independent of input), \n",
        "        output wise (same for every pixels in the output).\n",
        "            n ~ Normal(0, 1 / beta)\n",
        "            beta ~ Gamma(beta | shape=2, rate=2e-6)\n",
        "\n",
        "    Hyperparameters for weights and noise are pre-defined based on heuristic.\n",
        "\n",
        "    Given a deterministic `model`, initialize `n_samples` replicates\n",
        "    of the `model`. (plus `n_samples` of noise precision realizations)\n",
        "\n",
        "    `model` must implement `reset_parameters` method for the replicates\n",
        "    to have different initial realizations of uncertain parameters.\n",
        "\n",
        "    References:\n",
        "        Liu, Qiang, and Dilin Wang. \"Stein variational gradient descent:\n",
        "        A general purpose bayesian inference algorithm.\"\n",
        "        Advances In Neural Information Processing Systems. 2016.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The deterministic NN to be instantiated `n_samples` \n",
        "            times\n",
        "        data_loader (utils.data.DataLoader): For training and testing\n",
        "        n_samples (int): Number of samples for uncertain parameters\n",
        "    \"\"\"\n",
        "    def __init__(self, model, n_samples=20):\n",
        "        super(BayesNN, self).__init__()\n",
        "        if not isinstance(model, nn.Module):\n",
        "            raise TypeError(\"model {} is not a Module subclass\".format(\n",
        "                torch.typename(model)))\n",
        "\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "        # w_i ~ StudentT(w_i | mu=0, lambda=shape/rate, nu=2*shape)\n",
        "        # for efficiency, represent StudentT params using Gamma params\n",
        "        self.w_prior_shape = 1.\n",
        "        self.w_prior_rate = 0.05\n",
        "        \n",
        "        # noise variance 1e-6: beta ~ Gamma(beta | shape, rate)\n",
        "        self.beta_prior_shape = 2.\n",
        "        self.beta_prior_rate = 1.e-6\n",
        "\n",
        "        # replicate `n_samples` instances with the same network as `model`\n",
        "        instances = []\n",
        "        for i in range(n_samples):\n",
        "            new_instance = copy.deepcopy(model)\n",
        "            # initialize each model instance with their defualt initialization\n",
        "            # instead of the prior\n",
        "            new_instance.reset_parameters()\n",
        "            print('Reset parameters in model instance {}'.format(i))\n",
        "            instances.append(new_instance)\n",
        "        self.nnets = nn.ModuleList(instances)\n",
        "        del instances\n",
        "\n",
        "        # log precision (Gamma) of Gaussian noise\n",
        "        log_beta = Gamma(self.beta_prior_shape, \n",
        "                         self.beta_prior_rate).sample((self.n_samples,)).log()\n",
        "        for i in range(n_samples):\n",
        "            self.nnets[i].log_beta = Parameter(log_beta[i])\n",
        "\n",
        "        print('Total number of parameters: {}'.format(self._num_parameters()))\n",
        "\n",
        "    def _num_parameters(self):\n",
        "        count = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            # print(name)\n",
        "            count += param.numel()\n",
        "        return count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.nnets[idx]\n",
        "\n",
        "    @property\n",
        "    def log_beta(self):\n",
        "        return torch.tensor([self.nnets[i].log_beta.item() \n",
        "            for i in range(self.n_samples)], device=device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Computes all the `n_samples` NN output\n",
        "        Args:\n",
        "            input: N x iC x iH x iW\n",
        "\n",
        "        Return:\n",
        "            output: S x N x oC x oH x oW\n",
        "        \"\"\"\n",
        "        output = []\n",
        "        for i in range(self.n_samples):\n",
        "            output.append(self.nnets[i].forward(input))\n",
        "        output = torch.stack(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _log_joint(self, index, output, target, ntrain):\n",
        "        \"\"\"Log joint probability or unnormalized posterior for single model\n",
        "        instance. Ignoring constant terms for efficiency.\n",
        "        Can be implemented in batch computation, but memory is the bottleneck.\n",
        "        Thus here we trade computation for memory, e.g. using for loop.\n",
        "\n",
        "        Args:\n",
        "            index (int): model index, 0, 1, ..., `n_samples`\n",
        "            output (Tensor): B x oC x oD x oH x oW\n",
        "            target (Tensor): B x oC x oD x oH x oW\n",
        "            ntrain (int): total number of training data, mini-batch is used to\n",
        "                evaluate the log joint prob\n",
        "\n",
        "        Returns:\n",
        "            Log joint probability (zero-dim tensor)\n",
        "        \"\"\"\n",
        "        # Normal(target | output, 1 / beta * I)\n",
        "        log_likelihood = ntrain / output.size(0) * (\n",
        "                            - 0.5 * self.nnets[index].log_beta.exp()\n",
        "                            * (target - output).pow(2).sum()\n",
        "                            + 0.5 * target.numel() * self.nnets[index].log_beta)\n",
        "        # log prob of prior of weights, i.e. log prob of studentT\n",
        "        log_prob_prior_w = torch.tensor(0.).to(device)\n",
        "        for param in self.nnets[index].features.parameters():\n",
        "            log_prob_prior_w += \\\n",
        "                torch.log1p(0.5 / self.w_prior_rate * param.pow(2)).sum()\n",
        "        log_prob_prior_w *= -(self.w_prior_shape + 0.5)\n",
        "        # log prob of prior of log noise-precision (NOT noise precision)\n",
        "        log_prob_prior_log_beta = ((self.beta_prior_shape-1.0) * self.nnets[index].log_beta \\\n",
        "                    - self.nnets[index].log_beta.exp() * self.beta_prior_rate)\n",
        "        return log_likelihood + log_prob_prior_w + log_prob_prior_log_beta\n",
        "\n",
        "\n",
        "    def _compute_mse_nlp(self, input, target, size_average=True, out=False):\n",
        "        \"\"\"Evaluate the MSE and Negative Log Probability.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): (N, iC, iH, iW)\n",
        "            target (Tensor): (N, oC, oH, oW)\n",
        "            size_average (bool)\n",
        "            out (bool): If True, return output of `bayes_nn` w. `input`\n",
        "\n",
        "        Returns:\n",
        "            (mse, nlp) if `out` is False, o.w. (mse, nlp, output)\n",
        "            where output is of size (S, N, oC, oH, oW)\n",
        "        \"\"\"\n",
        "        # S x N x oC x oH x oW\n",
        "        output = self.forward(input)\n",
        "        # S x 1 x 1 x 1 x 1\n",
        "        log_beta = self.log_beta.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "        log_2pi_S = torch.tensor(0.5 * target[0].numel() * math.log(2 * math.pi)\n",
        "                       + math.log(self.n_samples), device=device)\n",
        "        # S x N\n",
        "        # print(log_beta.exp().shape)\n",
        "        # print(((target - output) ** 2).shape)\n",
        "        exponent = - 0.5 * (log_beta.exp() * ((target - output) ** 2)).view(\n",
        "            self.n_samples, target.size(0), -1).sum(-1) \\\n",
        "                    + 0.5 * target[0].numel() * self.log_beta.unsqueeze(-1)\n",
        "\n",
        "        # n = target[0].numel()\n",
        "        nlp = - log_sum_exp(exponent, dim=0).mean() + log_2pi_S\n",
        "        mse = ((target - output.mean(0)) ** 2).mean()\n",
        "\n",
        "        if not size_average:\n",
        "            mse *= target.numel()\n",
        "            nlp *= target.size(0)\n",
        "        if not out:\n",
        "            return mse, nlp\n",
        "        else:\n",
        "            return mse, nlp, output\n",
        "\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\"\n",
        "        Predictive mean and variance at x_test. (only average over w and beta)\n",
        "        Args:\n",
        "            x_test (Tensor): [N, *], test input\n",
        "        \"\"\"\n",
        "        # S x N x oC x oH x oW\n",
        "        y = self.forward(x_test)\n",
        "        y_pred_mean = y.mean(0)\n",
        "        # compute predictive variance per pixel\n",
        "        # N x oC x oH x oW\n",
        "        EyyT = (y ** 2).mean(0)\n",
        "        EyEyT = y_pred_mean ** 2\n",
        "        beta_inv = (- self.log_beta).exp()\n",
        "        y_pred_var = beta_inv.mean() + EyyT - EyEyT\n",
        "\n",
        "        return y_pred_mean, y_pred_var\n",
        "\n",
        "\n",
        "    def propagate(self, mc_loader):\n",
        "        \"\"\"\n",
        "        Mean and Variance statistics of predictive output distribution\n",
        "        averaging over the input distribution, i.e. uncertainty propagation.\n",
        "\n",
        "        First compute the conditional predictive mean and var given realizations\n",
        "        of uncertain surrogate; then compute the statistics of those conditional\n",
        "        statistics.\n",
        "\n",
        "        Args:\n",
        "            mc_loader (torch.utils.data.DataLoader): dataloader for the Monte \n",
        "                Carlo data (10,000 is used in this work)\n",
        "\n",
        "            S: num of samples\n",
        "            M: num of data\n",
        "            D: output dimensions\n",
        "        \"\"\"\n",
        "        # First compute conditional statistics\n",
        "        # S x N x oC x oH x oW\n",
        "        # self.cpu()\n",
        "        # x_test = x_test.cpu()\n",
        "        # print('here')\n",
        "\n",
        "        # S x oC x oH x oW\n",
        "        output_size = mc_loader.dataset[0][1].size()\n",
        "        cond_Ey = torch.zeros(self.n_samples, *output_size, device=device)\n",
        "        cond_Eyy = torch.zeros_like(cond_Ey)\n",
        "\n",
        "        for _, (x_mc, _) in enumerate(mc_loader):\n",
        "            x_mc = x_mc.to(device)\n",
        "            # S x B x oC x oH x oW            \n",
        "            y = self.forward(x_mc)\n",
        "            cond_Ey += y.mean(1)\n",
        "            cond_Eyy += y.pow(2).mean(1)\n",
        "        cond_Ey /= len(mc_loader)\n",
        "        cond_Eyy /= len(mc_loader)\n",
        "        beta_inv = (- self.log_beta).exp()\n",
        "        print('Noise variances: {}'.format(beta_inv))\n",
        "        \n",
        "        y_cond_pred_var = cond_Eyy - cond_Ey ** 2 \\\n",
        "                     + beta_inv.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # compute statistics of conditional statistics\n",
        "        return cond_Ey.mean(0), cond_Ey.var(0), \\\n",
        "               y_cond_pred_var.mean(0), y_cond_pred_var.var(0)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzEfjKZMa4zZ"
      },
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    #randn_like: Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.\n",
        "    #return: random gaussian sample from distribution with mu and exp(logvar/2)\n",
        "    return mu + eps*std\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.BatchNorm3d(in_features)]\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "            layers += [nn.Conv3d(in_features, filters, 3, 1, 1, bias=True)] # does not change state size\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)#, DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48, num_res_blocks=1,num_upsample=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # First layer. input size, inchannels x 2 x 8 x 16\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # state size. filters x 2 x 8 x 16\n",
        "        # Residual blocks\n",
        "        self.res_block1 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks+1)])\n",
        "        self.transup1 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(4, 21, 41), mode='nearest'),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=1, padding=1), #does not change state size\n",
        "        )\n",
        "        self.res_block2 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        self.transup2 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(8, 41, 81), mode='nearest'),\n",
        "            nn.Conv3d(filters, outchannels, kernel_size=3, stride=1, padding=(0,1,1)), # reduce the first dimension by 2\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        # x: in_channels x 2 x 8 x 16\n",
        "        out1 = self.conv1(z)          # filters x 2 x 8 x 16\n",
        "        out2 = self.res_block1(out1)   # filters x 2 x 8 x 16\n",
        "        out = torch.add(out1, out2)   # filters x 2 x 8 x 16\n",
        "        out3 = self.transup1(out)      # filters x 4 x 16 x 32\n",
        "        out4 = self.res_block2(out3)   # filters x 4 x 16 x 32\n",
        "\n",
        "        img = self.transup2(out4)     # outchannels x 6 x 32 x 64\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params= 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqTAHiwccBzC"
      },
      "source": [
        "# load CAAE and Bayesian DenseED models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBNGJ-pCbOo4",
        "outputId": "e4db9b69-fd44-4c4a-e9f5-651ec5d87769"
      },
      "source": [
        "'''Load the CAAE model first'''\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "n_train = 23000\n",
        "n_test = 100\n",
        "batch_size = 64\n",
        "n_epochs = 50\n",
        "lr = 0.0002 ## adam learning rate\n",
        "lw = 0.01 ## \"adversarial loss weight\"\n",
        "\n",
        "current_dir = \"/content/drive/MyDrive/react_inverse/CAAE/\"\n",
        "date = 'experiments/Feb_14_CAAE3D'\n",
        "exp_dir = current_dir + date + \"/N{}_Bts{}_Eps{}_lr{}_lw{}\".\\\n",
        "    format(n_train, batch_size, n_epochs, lr, lw)\n",
        "\n",
        "output_dir = exp_dir + \"/predictions\"\n",
        "model_dir = exp_dir\n",
        "\n",
        "nf, d, h, w = 2, 2, 11, 21\n",
        "\n",
        "# Initialize decoder\n",
        "decoder = Decoder(inchannels=nf)\n",
        "decoder.load_state_dict(torch.load(model_dir + '/AAE_decoder_epoch{}.pth'.format(n_epochs)))\n",
        "if cuda:\n",
        "    decoder.cuda()\n",
        "\n",
        "decoder.eval()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (conv1): Conv3d(2, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  (res_block1): Sequential(\n",
              "    (0): ResidualInResidualDenseBlock(\n",
              "      (dense_blocks): Sequential(\n",
              "        (0): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1): ResidualInResidualDenseBlock(\n",
              "      (dense_blocks): Sequential(\n",
              "        (0): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transup1): Sequential(\n",
              "    (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Upsample(size=(4, 21, 41), mode=nearest)\n",
              "    (3): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "  )\n",
              "  (res_block2): Sequential(\n",
              "    (0): ResidualInResidualDenseBlock(\n",
              "      (dense_blocks): Sequential(\n",
              "        (0): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (1): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "        (2): DenseResidualBlock(\n",
              "          (b1): Sequential(\n",
              "            (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b2): Sequential(\n",
              "            (0): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b3): Sequential(\n",
              "            (0): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(144, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b4): Sequential(\n",
              "            (0): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(192, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "          (b5): Sequential(\n",
              "            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (1): ReLU(inplace=True)\n",
              "            (2): Conv3d(240, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transup2): Sequential(\n",
              "    (0): BatchNorm3d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Upsample(size=(8, 41, 81), mode=nearest)\n",
              "    (3): Conv3d(48, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qesJdHz7dVv_",
        "outputId": "8bad73af-15a5-4d01-b2aa-2e8897566d82"
      },
      "source": [
        "exp_dir = \"/content/drive/MyDrive/react_inverse/Bayesian_DenseED\"\n",
        "all_over_again = 'Apr_04'\n",
        "n_samples = 20\n",
        "n_train = 1000\n",
        "batch_size = 40\n",
        "lr = 0.005\n",
        "lr_noise = 0.01\n",
        "n_epochs = 300\n",
        "ckpt_epoch = None\n",
        "run_dir = exp_dir + '/' + all_over_again\\\n",
        "    + '/ns{}_ntr{}_bt{}_lr{}_lrn{}_ep{}'.format(\n",
        "        n_samples, n_train, batch_size, lr,\n",
        "        lr_noise, n_epochs)\n",
        "pred_dir = run_dir + '/predictions'\n",
        "ckpt_dir = run_dir + '/checkpoints'\n",
        "nic = 3\n",
        "noc = 2\n",
        "blocks = [3,6,3]\n",
        "growth_rate = 40\n",
        "init_features = 48\n",
        "drop_rate = 0.\n",
        "bn_size = 8\n",
        "bottleneck = False\n",
        "\n",
        "dense_ed = DenseED(\n",
        "    in_channels=nic, \n",
        "    out_channels=noc, \n",
        "    blocks=blocks,\n",
        "    growth_rate=growth_rate, \n",
        "    num_init_features=init_features,\n",
        "    drop_rate=drop_rate,\n",
        "    bn_size=bn_size,\n",
        "    bottleneck=bottleneck,\n",
        ")\n",
        "# print(dense_ed)\n",
        "# Bayesian NN\n",
        "bayes_nn = BayesNN(dense_ed, n_samples=n_samples).to(device)\n",
        "# load the pre-trained model\n",
        "if ckpt_epoch is not None:\n",
        "    checkpoint = ckpt_dir + '/model_epoch{}.pth'.format(ckpt_epoch)\n",
        "else:\n",
        "    checkpoint = ckpt_dir + '/model_epoch{}.pth'.format(n_epochs)\n",
        "bayes_nn.load_state_dict(torch.load(checkpoint))\n",
        "print('Loaded pre-trained model: {}'.format(checkpoint))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # training on GPU or CPU\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reset parameters in model instance 0\n",
            "Reset parameters in model instance 1\n",
            "Reset parameters in model instance 2\n",
            "Reset parameters in model instance 3\n",
            "Reset parameters in model instance 4\n",
            "Reset parameters in model instance 5\n",
            "Reset parameters in model instance 6\n",
            "Reset parameters in model instance 7\n",
            "Reset parameters in model instance 8\n",
            "Reset parameters in model instance 9\n",
            "Reset parameters in model instance 10\n",
            "Reset parameters in model instance 11\n",
            "Reset parameters in model instance 12\n",
            "Reset parameters in model instance 13\n",
            "Reset parameters in model instance 14\n",
            "Reset parameters in model instance 15\n",
            "Reset parameters in model instance 16\n",
            "Reset parameters in model instance 17\n",
            "Reset parameters in model instance 18\n",
            "Reset parameters in model instance 19\n",
            "Total number of parameters: 63578420\n",
            "Loaded pre-trained model: /content/drive/MyDrive/react_inverse/Bayesian_DenseED/Apr_04/ns20_ntr1000_bt40_lr0.005_lrn0.01_ep300/checkpoints/model_epoch300.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjh2aQ31fZi0"
      },
      "source": [
        "# ILUES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEQHpJb5eQGv"
      },
      "source": [
        "class ILUES:\n",
        "\n",
        "    def ilues_select(self,Par, x, y):\n",
        "        self.Par = Par\n",
        "        Cd = np.eye(Par.Nobs)\n",
        "        for i in range(Par.Nobs):\n",
        "            Cd[i, i] = Par.sd[i]**2 #covariance matrix for the error\n",
        "\n",
        "        meanxf = np.tile(np.mean(x1, axis=1, keepdims=True), (1, Par.Ne))\n",
        "        Cm = np.matmul((x1 - meanxf), (x1 - meanxf).T)/(Ne - 1) # auto-covariance of the prior parameters\n",
        "        \n",
        "        J1 = np.zeros((Par.Ne,1))\n",
        "        for i in range(Par.Ne):\n",
        "            J1[i,] = np.matmul((yf[:,i]-Par.obs).T/Cd, (yf[:,i]-Par.obs))[0,0]\n",
        "\n",
        "        xa = np.zeros(x1.shape)  # define the updated ensemble   \n",
        "        for j in range(Par.Ne):\n",
        "            xa[:,j] = self.local_update(xf,yf,Cm,sd,obs,alpha,beta,J1,j)\n",
        "        return xa\n",
        "\n",
        "\n",
        "    def local_update(self,xf,yf,Cm,sd,obs,alpha,beta,J1,jj):\n",
        "        # The local updating scheme used in ILUES\n",
        "        Ne = xf.shape[1]\n",
        "        xr = xf[:, jj]\n",
        "        xr = np.tile(np.reshape(xr, (-1, 1)), (1, Ne))\n",
        "        J = np.matmul((xf-xr).T/Cm, (xf-xr))\n",
        "        J2 = np.diag(J)\n",
        "        J3 = J1/np.max(J1) + J2/np.max(J2)\n",
        "        M = np.ceil(Ne*alpha)\n",
        "\n",
        "        J3min, index = np.min(J3), np.unravel_index(np.argmin(J3, axis=None), J3.shape)\n",
        "        xl = xf[:, index]\n",
        "        yl = yf[:, index]\n",
        "        alpha_ = J3min / J3\n",
        "        alpha_[index] = 0\n",
        "        index1 = self.RouletteWheelSelection(alpha_, M-1)\n",
        "        xl1 = xf[:, index1]\n",
        "        yl1 = yf[:, index1]\n",
        "        xl = np.asarray([xl, xl1])\n",
        "        yl = np.asarray([yl, yl1])\n",
        "        xu =  self.update_para(xl,yl,self.Par.para_range,sd*beta,obs)\n",
        "        a = time.time()\n",
        "        np.random.seed(int(a))\n",
        "        xest = xu[:,np.random.permutation(M)]\n",
        "        return xest\n",
        "\n",
        "    def RouletteWheelSelection(self,V,m):\n",
        "        '''\n",
        "        Input:\n",
        "              V           -----fitness criterion\n",
        "              m           -----number of individuals to be chosen\n",
        "        Output:\n",
        "              index       -----index of the chosen individuals\n",
        "        '''\n",
        "        n = V.shape[1]\n",
        "        if np.max(V)==0 and np.min(V)==0:\n",
        "            index=np.ceil(np.random.uniform(size=(1,m))*n)\n",
        "        else:\n",
        "            temindex= np.nonzero(V)\n",
        "            n=len(temindex)\n",
        "            V=V[temindex]\n",
        "\n",
        "            V=np.cumsum(V)/np.sum(V)\n",
        "\n",
        "            pp=np.random.uniform(size=(1,m))\n",
        "            index = []\n",
        "            for i in range(m):\n",
        "\n",
        "                while True:\n",
        "                    flag = True\n",
        "                    for j in range(n):\n",
        "                        if pp[i] < V[j]:\n",
        "                            index.append(j)\n",
        "                            V[j] = 0\n",
        "                            flag = False\n",
        "                            break\n",
        "\n",
        "                    if flag:\n",
        "                        pp[i] = np.random.uniform()\n",
        "                    else:\n",
        "                        break\n",
        "        return np.array(index)\n",
        "\n",
        "\n",
        "    def update_para(self,xf,yf,para_range, sd,obs):\n",
        "        #Update the model parameters via the ensemble smoother\n",
        "        para_range = se\n",
        "        Npar = xf.shape[0]\n",
        "        Ne = xf.shape[1]\n",
        "        Nobs = len(obs)\n",
        "\n",
        "        Cd = np.eye(Nobs)\n",
        "        for i in range(Nobs):\n",
        "            Cd[i,i] = sd(i)**2\n",
        "        meanxf = np.tile(np.mean(xf, axis=1, keepdims=True), (1, Ne))\n",
        "        meanyf = np.tile(np.mean(yf, axis=1, keepdims=True), (1, Ne))\t\n",
        "        Cxy = np.matmul((xf - meanxf), (yf - meanyf).T)/(Ne - 1)\n",
        "        Cyy = np.matmul((yf - meanyf), (yf - meanyf).T)/(Ne - 1)\n",
        "\n",
        "        kgain = linalg.lstsq((Cyy + Cd).T, Cxy.T)[0].T ##Cxy/(...), A/B = (B'\\A')', b/a: linalg.lstsq(a.T, b.T)\n",
        "        obse = np.tile(np.reshape(obs,(-1,1)),(1,Ne)) +\\\n",
        "          np.random.normal(np.zeros((Nobs,Ne)),np.tile(np.reshape(sd, (-1, 1)),(1,Ne)))\n",
        "        xa = xf + np.matmul(kgain, (obse - yf))\n",
        "\n",
        "        ##when the updated parameters exceed the range\t\n",
        "        for i in range(Ne):\n",
        "            for j in range(Npar):\n",
        "                if xa[j,i] > para_range[1, j]:\n",
        "                    xa[j,i] = (para_range[1, j] + xf[j,i])/2\n",
        "                elif xa[j,i] < para_range[0, j]:\n",
        "                    xa[j,i] = (para_range[0, j] + xf[j,i])/2\n",
        "        \n",
        "        return xa"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIe13wzU16TS"
      },
      "source": [
        "def plot_3d(data, title='', cut=None):\n",
        "    data = np.transpose(data, (2, 1, 0))\n",
        "    data = np.flip(data, axis=2)\n",
        "    filled = np.ones(data.shape)\n",
        "    if cut is not None:\n",
        "        filled[cut[2]:, :cut[1], (6-cut[0]):] = 0\n",
        "    x, y, z = np.indices(np.array(filled.shape) + 1)\n",
        "    \n",
        "    v1 = np.linspace(np.min(data),np.max(data), 8, endpoint=True)\n",
        "    norm = matplotlib.colors.Normalize(vmin=np.min(data), vmax=np.max(data))\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    ax = plt.axes(projection='3d')\n",
        "    ax.voxels(x, y, z, filled, facecolors=plt.cm.jet(norm(data)), edgecolors=None, \n",
        "              )\n",
        "    ax.set_box_aspect((250, 125, 50))\n",
        "    \n",
        "    m = cm.ScalarMappable(cmap=plt.cm.jet, norm=norm)\n",
        "    m.set_array([])\n",
        "    fig.colorbar(m, ax=ax, fraction=0.015, pad=0.04,ticks=v1,)\n",
        "    ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "    # ax.set_title(title)\n",
        "    # plt.savefig(title+'.pdf',bbox_inches='tight')\n",
        "    return fig\n",
        "# fig = plot_3d(log_K[0,], title='', cut=[3, 13+1, 20-1])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-rLofxpkoLQ"
      },
      "source": [
        "# ILUES support functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HREyH4hgeQ_u"
      },
      "source": [
        "def to_numpy(input):\n",
        "    if isinstance(input, torch.Tensor):\n",
        "        return input.cpu().detach().numpy()\n",
        "    elif isinstance(input, np.ndarray):\n",
        "        return input\n",
        "    else:\n",
        "        raise TypeError('Unknown type of input, expected torch.Tensor or '\\\n",
        "            'np.ndarray, but got {}'.format(type(input)))\n",
        "\n",
        "def run_surrogate(X, Par, model):\n",
        "    ya = np.zeros((Par.Ne, Par.Nobs))\n",
        "    model.eval()\n",
        "    start = time.time()\n",
        "    log_K, source = gen_input4net(X, Par)\n",
        "    end1 = time.time()\n",
        "    print('Generating logK and source took ', end1-start, ' s.')\n",
        "    for ind in range(Par.Ne):\n",
        "        x = np.zeros((1,3,6,41,81))           # three input channels: hydraulic conductivity field, source term, previous concentration field\n",
        "        y = np.zeros( (Par.Nt,2,6,41,81)) # two output channles: concentration and head fields\n",
        "        y_i_1 = np.zeros((6,41,81))   # y_0 = 0\n",
        "\n",
        "        for i in range(Par.Nt):\n",
        "            x[0,0,:,:,:] = y_i_1          # the i-1)^th predicted concentration field, which is treated as an input channel\n",
        "            x[0,1,:,:,:] = log_K[ind]           # hydraulic conductivity\n",
        "            x[0,2,:,:,:] = source[ind][i]      # source rate\n",
        "            x_tensor = Tensor(x)\n",
        "            with torch.no_grad():\n",
        "                y_hat = model.forward(x_tensor).mean(0)\n",
        "            to_numpy(x_tensor)\n",
        "            y_hat = y_hat.data.cpu().numpy()\n",
        "            y[i] = y_hat\n",
        "            y_i_1 = y_hat[0,0,:,:,:]      # the updated (i-1)^th predicted concentration field\n",
        "\n",
        "        y_pred = np.full( (Par.Nt + 1,6,41,81), 0.0)\n",
        "        y_pred[:Par.Nt] = y[:,0]   # the concentration fields at Nt time instances\n",
        "        y_pred[Par.Nt]  = y[0,1]   # the hydraulic head field\n",
        "        ya[ind, :] = np.reshape(get_obs(sensor, y_pred),(-1,))  # get the simulated outputs at observation locations using interpolation\n",
        "    end2 = time.time()\n",
        "    print('Running {} samples took '.format(Par.Ne), end2-end1, ' s.')\n",
        "    return ya\n",
        "   \n",
        "def gen_input4net(X, Par, batch_size=100):\n",
        "    '''generate batch input'''\n",
        "    ## log conductivity field\n",
        "    log_K = np.zeros((Par.Ne, 6, 41, 81))\n",
        "    latent_z = X[:Par.Nlat, :] ## 924, Ne\n",
        "    assert Par.Ne % batch_size ==0, 'Number of samples cannot be divided by the batch size.'\n",
        "    for i in range(int(Par.Ne/batch_size)):\n",
        "        batch_z = latent_z[:, i*batch_size:(i+1)*batch_size].copy()\n",
        "        batch_z = torch.reshape(\n",
        "          Tensor(batch_z), (-1, nf, d, h, w)\n",
        "          )\n",
        "        log_K[i*batch_size:(i+1)*batch_size,] = to_numpy(decoder(batch_z)).reshape(-1, 6, 41, 81)\n",
        "        to_numpy(batch_z)\n",
        "    ## source loc\n",
        "    y_wel_samp = X[Par.Nlat, :]\n",
        "    x_wel_samp = X[Par.Nlat+1, :]\n",
        "    Sy_id, Sx_id = step_loc(y_wel_samp, x_wel_samp)\n",
        "    ## source rate\n",
        "    source_rate = X[Par.Nlat+2:, :] #shape: 5,Ne\n",
        "    source = np.zeros((Par.Ne, Par.Nt, 6, 41, 81,))\n",
        "    for j in range(Par.Nt_re): #j'th timestep of release\n",
        "        for i in range(Par.Ne): #i'th sample\n",
        "            source[i, j, 3, Sy_id[i], Sx_id[i]] = source_rate[j,i]\n",
        "\n",
        "    return log_K, source\n",
        "\n",
        "def get_obs(sensor, y_pred):\n",
        "\n",
        "    y_sim_obs = []\n",
        "    for y in y_pred:\n",
        "        y_sim_obs.append(y[Par.sensor])\n",
        "\n",
        "    return y_sim_obs\n",
        "\n",
        "def gen_init(Par, seed=888):\n",
        "    x = np.zeros((Par.Npar, Par.Ne))\n",
        "    np.random.seed(seed)\n",
        "    ## log_K\n",
        "    x[:Par.Nlat, :] = np.random.randn(Par.Nlat, Par.Ne)\n",
        "    ## release locations\n",
        "    y_wel = np.array([125, 125*3, 125*5, 125*7, 125*9])\n",
        "    x_wel = np.array([125, 125*3, 125*5, 125*7])\n",
        "    # wells = {i: [y_wel[i], x_wel[i]] for i in range(len(y_wel))}\n",
        "    y_wel_samp = np.random.choice(y_wel, Par.Ne)\n",
        "    x_wel_samp = np.random.choice(x_wel, Par.Ne)\n",
        "\n",
        "    x[Par.Nlat,:] = y_wel_samp\n",
        "    x[Par.Nlat+1,:] = x_wel_samp\n",
        "    ## release concentration for 5 periods\n",
        "    q = np.random.uniform(low=100, high=1000, size=(5, Par.Ne)).astype(int)\n",
        "    x[Par.Nlat+2:,:] = q\n",
        "    return x\n",
        "\n",
        "'''MAKE A FUNCTION TO STEPWISE THE WELL LOCATIONS'''\n",
        "def step_loc(y_loc, x_loc):\n",
        "    '''\n",
        "    x_loc: (Ne, ), convert from meters to index,\n",
        "    y_loc: (Ne, ), convert from meters to index.\n",
        "    '''\n",
        "    dy = 1250/40\n",
        "    dx = 2500/80\n",
        "    y_wel = np.array([125, 125*3, 125*5, 125*7, 125*9])\n",
        "    x_wel = np.array([125, 125*3, 125*5, 125*7])\n",
        "    N = len(x_loc)\n",
        "    x_dist_wel = np.array(\n",
        "        [\n",
        "            [np.abs(x_loc[i] - x_wel[j]) for j in range(len(x_wel))] \n",
        "            for i in range(N)\n",
        "        ]\n",
        "    )\n",
        "    y_dist_wel = np.array(\n",
        "        [\n",
        "            [np.abs(y_loc[i] - y_wel[j]) for j in range(len(y_wel))] \n",
        "            for i in range(N)\n",
        "        ]\n",
        "    )\n",
        "    y_wel = (y_wel)//dy\n",
        "    x_wel = (x_wel)//dx\n",
        "    y_loc_ind = np.array([y_wel[np.argmin(y_dist_wel, axis=1)[i]] for i in range(N)])\n",
        "    x_loc_ind = np.array([x_wel[np.argmin(x_dist_wel, axis=1)[i]] for i in range(N)])\n",
        "    \n",
        "    return y_loc_ind.astype(int), x_loc_ind.astype(int)\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny2tIY70Cuvi"
      },
      "source": [
        "## load the measurements, meas[:,0]: the measurements, meas[:, 1]: sigma for meas error.\n",
        "with open('/content/drive/MyDrive/react_inverse/dense_ed_3d/sensor_loc.pkl', 'rb') as file:\n",
        "    sensor = pk.load(file)\n",
        "with open('/content/drive/MyDrive/react_inverse/ILUES/output.pk', 'rb') as file:\n",
        "    [conc, heads] = pk.load(file)\n",
        "conc, heads = np.array(conc), np.array(heads)\n",
        "conc[np.where(conc<0)] = 0\n",
        "meas_sig = [0.05*conc[i][sensor] for i in range(len(conc))] + [0.05*heads[sensor]]\n",
        "meas_orig = [conc[i][sensor] for i in range(len(conc))] + [heads[sensor]]\n",
        "np.random.seed(888)\n",
        "meas_data = [meas_orig[i] + np.random.normal(0, meas_sig[i]) for i in range(len(meas_orig))]\n",
        "meas = np.array(\n",
        "    [np.reshape(meas_data, (-1,)), np.reshape(meas_sig, (-1))]\n",
        ").T"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jUHJObXIKQA"
      },
      "source": [
        "# for j in range(len(meas_data[0])):\n",
        "#     plt.plot([meas_data[i][j] for i in range(len(meas_data))])\n",
        "def vis_sensors(meas,sensor, t):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    fig.suptitle(t)\n",
        "    for i in range(len(meas[0])):\n",
        "        sen = [None]*len(meas)\n",
        "        for j in range(len(meas)-1):\n",
        "            sen[j] = meas[j][i]\n",
        "        ax = fig.add_subplot(12,10,i+1)\n",
        "        ax.plot(sen)\n",
        "        ax.set_title(str(sensor[0][i])+','+str(sensor[1][i])+','+str(sensor[2][i]))\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.88)\n",
        "    name = t + '.pdf'\n",
        "    fig.savefig(name, format='pdf')\n",
        "    plt.show()\n",
        "\n",
        "vis_sensors(meas_data, np.where(sensor), 'conc_sensor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLKMooafKvmd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13125e9a-224f-4a28-e77e-3e774bd645b7"
      },
      "source": [
        "np.array(meas_data).shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZPMfmJGmlPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754ff9e1-82ef-4f7e-f441-788bb1cdfb0c"
      },
      "source": [
        "# with open('obs_sd.pkl', 'rb') as file:\n",
        "#     meas = pkl.load(file)\n",
        "Par=AttrDict()\n",
        "Par.obs = meas[:,0] # observations\n",
        "Par.sd  = meas[:,1] # standard deviations of the observation error\n",
        "Par.Nobs = meas.shape[0]\n",
        "Par.sensor = sensor\n",
        "Par.Nlat = nf*d*h*w\n",
        "Par.Nt = 10\n",
        "Par.Nt_re = 5\n",
        "Par.Npar = Par.Nlat + 2 + Par.Nt_re \n",
        "\n",
        "Par.N_iter = 20\n",
        "Par.alpha = 0.1  # a scalar within [0 1]\n",
        "Par.Ne = 600\n",
        "Par.beta = np.sqrt(Par.N_iter)\n",
        "Par.para_range = np.asarray(\n",
        "    [\n",
        "        [0, 0] + [100 for i in range(Par.Nt_re)] + [-5 for i in range(Par.Nlat)],\n",
        "        [1200, 812.5] + [1000 for i in range(Par.Nt_re)] + [5 for i in range(Par.Nlat)]\n",
        "    ]\n",
        ")\n",
        "X = gen_init(Par)\n",
        "print('X shape: ', X.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x shape:  (931, 600)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpkkviUv_Uof",
        "outputId": "70c4fe65-ef99-4280-9ebd-cc71dc96668e"
      },
      "source": [
        "ya = run_surrogate(X, Par, bayes_nn)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating logK and source took  5.382914304733276  s.\n",
            "Running 600 samples took  1030.2767112255096  s.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}