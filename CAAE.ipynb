{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP/Yblgmji4kpcZ/VKM776G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZitongZhou/react_inverse/blob/master/CAAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S8Bd4yZgsZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4a820f-5931-4840-8e54-9aaf49cba145"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "import pickle as pk\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roo9l7s3UYRa"
      },
      "source": [
        "# CAAE 3d models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t44ZwK0bYK3G"
      },
      "source": [
        "## model definition part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_cb8ucUWPv"
      },
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    #randn_like: Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.\n",
        "    #return: random gaussian sample from distribution with mu and exp(logvar/2)\n",
        "    return mu + eps*std\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.BatchNorm3d(in_features)]\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "            layers += [nn.Conv3d(in_features, filters, 3, 1, 1, bias=True)] # does not change state size\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)#, DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, inchannels=1, outchannels=2, filters=48, num_res_blocks=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        # input size, inchannels x 6 x 41 x 81\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=2, padding=1)\n",
        "        # state size. filters x 3 x 21 x 41\n",
        "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        # state size. filters x 3 x 21 x 41\n",
        "        self.trans = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        # state size. filters x 2 x 11 x 21\n",
        "        self.mu = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False) #does not change state size.\n",
        "        self.logvar = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False) #does not change state size.\n",
        "\n",
        "    def forward(self, img):\n",
        "        # img: inchannels x 6 x 41 x 81\n",
        "        out1 = self.conv1(img)        # filters x 3 x 21 x 41\n",
        "        out2 = self.res_blocks(out1)   # filters x 3 x 21 x 41\n",
        "        out3 = self.trans(out2)        # filters x 2 x 11 x 21\n",
        "\n",
        "        mu, logvar = self.mu(out3), self.logvar(out3)\n",
        "        z = reparameterization(mu, logvar) # latent dimension: outchannels x 2 x 11 x 21\n",
        "        return z\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48, num_res_blocks=1,num_upsample=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # First layer. input size, inchannels x 2 x 8 x 16\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # state size. filters x 2 x 8 x 16\n",
        "        # Residual blocks\n",
        "        self.res_block1 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks+1)])\n",
        "        self.transup1 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(4, 21, 41), mode='nearest'),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=1, padding=1), #does not change state size\n",
        "        )\n",
        "        self.res_block2 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        self.transup2 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(8, 41, 81), mode='nearest'),\n",
        "            nn.Conv3d(filters, outchannels, kernel_size=3, stride=1, padding=(0,1,1)), # reduce the first dimension by 2\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        # x: in_channels x 2 x 8 x 16\n",
        "        out1 = self.conv1(z)          # filters x 2 x 8 x 16\n",
        "        out2 = self.res_block1(out1)   # filters x 2 x 8 x 16\n",
        "        out = torch.add(out1, out2)   # filters x 2 x 8 x 16\n",
        "        out3 = self.transup1(out)      # filters x 4 x 16 x 32\n",
        "        out4 = self.res_block2(out3)   # filters x 4 x 16 x 32\n",
        "\n",
        "        img = self.transup2(out4)     # outchannels x 6 x 32 x 64\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params= 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (inchannels) x 2 x 11 x 21\n",
        "            nn.Conv3d(inchannels, filters, 3, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 6 x 11\n",
        "            nn.Conv3d(filters, filters, 3, 1, 1, bias=True),\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 6 x 11\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(filters * 6 * 11,128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(128, outchannels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output1 = self.fc1(output)\n",
        "        output2 = self.fc2(output1)\n",
        "        return output2\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_5E4jwXsJf"
      },
      "source": [
        "## data loading and plotting block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shu_Pl3HXptF"
      },
      "source": [
        "def load_data(opt):\n",
        "    with open('/content/drive/MyDrive/react_inverse/training_data/kds.pkl', 'rb') as file:\n",
        "      kds = np.expand_dims(np.asarray(pk.load(file)), axis=1)\n",
        "    print('Total number of conductivity images:', len(kds))\n",
        "\n",
        "    x_train = kds[:opt.n_train]\n",
        "    x_test = kds[opt.n_train: opt.n_train + opt.n_test]\n",
        "\n",
        "    print(\"total training data shape: {}\".format(x_train.shape))\n",
        "\n",
        "    data = torch.utils.data.TensorDataset(torch.FloatTensor(x_train))\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=opt.batch_size,\n",
        "                                              shuffle=True, num_workers=int(2))\n",
        "    return data_loader, x_test\n",
        "\n",
        "def simple_plot(c_map, title=''):\n",
        "    nx = 81\n",
        "    ny = 41\n",
        "    Lx = 2500\n",
        "    Ly = 1250\n",
        "\n",
        "    x = np.linspace(0, Lx, nx)\n",
        "    y = np.linspace(0, Ly, ny)\n",
        "    X,Y = np.meshgrid(x, y)\n",
        "    fig, axs = plt.subplots(1,1)\n",
        "#        axs.set_xlabel('x(m)')\n",
        "#        axs.set_ylabel('y(m)')\n",
        "    axs.set_xlim(0,Lx)\n",
        "    axs.set_ylim(0,Ly)\n",
        "    c01map = axs.imshow(c_map, cmap='jet',\n",
        "              extent=[x.min(), x.max(), y.min(), y.max()],\n",
        "              vmin=c_map.min(), vmax = c_map.max(),\n",
        "              origin='lower')\n",
        "    fig.colorbar(c01map, ax=axs,shrink=0.62)\n",
        "    name = title + '.pdf'\n",
        "    plt.title(title)\n",
        "#         fig.savefig('images/'+name, format='pdf',bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def plot_pred(samples, epoch, idx, output_dir):\n",
        "    Ncol = 3\n",
        "    Nrow = samples.shape[0] // Ncol\n",
        "\n",
        "    fig, axes = plt.subplots(Nrow, Ncol, figsize=(Ncol*4, Nrow*2.1))\n",
        "    fs = 16 # font size\n",
        "    for j, ax in enumerate(fig.axes):\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j < samples.shape[0]:\n",
        "\n",
        "            cax = ax.imshow(samples[j], cmap='jet', origin='lower', vmin=0, vmax=5)\n",
        "            cbar = plt.colorbar(cax, ax=ax, fraction=0.025, pad=0.04,\n",
        "                            format=ticker.ScalarFormatter(useMathText=True))\n",
        "            cbar.formatter.set_powerlimits((0, 0))\n",
        "            cbar.ax.yaxis.set_offset_position('left')\n",
        "            cbar.update_ticks()\n",
        "            cbar.ax.tick_params(axis='both', which='both', length=0)\n",
        "            cbar.ax.yaxis.get_offset_text().set_fontsize(fs-3)\n",
        "            cbar.ax.tick_params(labelsize=fs-2)\n",
        "\n",
        "    plt.savefig(output_dir+'/epoch_{}_{}.png'.format(epoch,idx), bbox_inches='tight',dpi=600)\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(\"epoch {}, done printing\".format(epoch))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFwSG0DgYHN3"
      },
      "source": [
        "## training part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhaCZPXUngE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f8c652-0eb3-4f41-c1e8-fed389558e5d"
      },
      "source": [
        "os.makedirs(\"images_CAAE\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--current-dir\", type=str, default=\"/content/drive/MyDrive/react_inverse/CAAE/\", help=\"data directory\")\n",
        "parser.add_argument(\"--n-epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
        "parser.add_argument('--n-train', type=int, default=23000, help='number of training data')\n",
        "parser.add_argument('--n-test', type=int, default=4000, help='number of training data')\n",
        "parser.add_argument(\"--batch-size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--lw\", type=float, default=0.01, help=\"adversarial loss weight\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--sample-interval\", type=int, default=10, help=\"interval between image sampling\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "date = 'experiments/Feb_14_CAAE3D'\n",
        "exp_dir = opt.current_dir + date + \"/N{}_Bts{}_Eps{}_lr{}_lw{}\".\\\n",
        "    format(opt.n_train, opt.batch_size, opt.n_epochs, opt.lr, opt.lw)\n",
        "\n",
        "output_dir = exp_dir + \"/predictions\"\n",
        "model_dir = exp_dir\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# loss functions\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "pixelwise_loss = torch.nn.L1Loss()\n",
        "\n",
        "nf, d, h, w = 2, 2, 11, 21\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "encoder = Encoder(outchannels=nf)\n",
        "decoder = Decoder(inchannels=nf)\n",
        "discriminator = Discriminator(inchannels=nf)\n",
        "\n",
        "print(\"number of parameters: {}\".format(encoder._n_parameters()+decoder._n_parameters()+discriminator._n_parameters()))\n",
        "\n",
        "if cuda:\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    pixelwise_loss.cuda()\n",
        "\n",
        "dataloader, x_test = load_data(opt)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(encoder.parameters(), decoder.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    n_samples = 1\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (n_samples, nf, d, h, w))))\n",
        "    gen_imgs = decoder(z)\n",
        "    samples = np.squeeze(gen_imgs.data.cpu().numpy())\n",
        "    plot_pred(samples,epoch,0,output_dir)\n",
        "\n",
        "    idx = np.random.choice(opt.n_test, 1, replace=False)\n",
        "    real_imgs = x_test[[idx]]\n",
        "    real_imgs = (torch.FloatTensor(real_imgs)).cuda()\n",
        "    encoded_imgs = encoder(real_imgs)\n",
        "    decoded_imgs = decoder(encoded_imgs)\n",
        "    samples_gen  = np.squeeze(decoded_imgs.data.cpu().numpy())\n",
        "    samples_real = np.squeeze(real_imgs.data.cpu().numpy())\n",
        "\n",
        "    samples = np.vstack((samples_real[:3],samples_gen[:3],samples_real[3:],samples_gen[3:]))\n",
        "    plot_pred(samples,epoch+1,idx,output_dir)\n",
        "\n",
        "start = time.time()\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "for epoch in range(1,opt.n_epochs+1):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for i, (imgs,) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.shape[0],1).fill_(1.0), requires_grad=False)\n",
        "        fake  = Variable(Tensor(imgs.shape[0],1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        encoded_imgs = encoder(real_imgs)\n",
        "        decoded_imgs = decoder(encoded_imgs)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss_a = adversarial_loss(discriminator(encoded_imgs), valid)\n",
        "        g_loss_c = pixelwise_loss(decoded_imgs, real_imgs)\n",
        "\n",
        "        g_loss = opt.lw * g_loss_a + g_loss_c\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as discriminator ground truth\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], nf, d, h, w))))\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(z), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(encoded_imgs.detach()), fake)\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    if (epoch) % opt.sample_interval == 0:\n",
        "        test(epoch)\n",
        "\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f /G_A loss: %f/ G_C loss: %f]\"\n",
        "        % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), g_loss_a.item(), g_loss_c.item())\n",
        "    )\n",
        "\n",
        "torch.save(decoder.state_dict(), model_dir + '/AAE_decoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(encoder.state_dict(), model_dir + '/AAE_encoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(discriminator.state_dict(), model_dir + '/AAE_discriminator_epoch{}.pth'.format(opt.n_epochs))\n",
        "print('time for training:', time.time()-start)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, current_dir='/content/drive/MyDrive/react_inverse/CAAE/', f='/root/.local/share/jupyter/runtime/kernel-d04df7a5-6eb7-43d3-b659-8f6ed71a6815.json', lr=0.0002, lw=0.01, n_epochs=50, n_test=4000, n_train=23000, sample_interval=10)\n",
            "number of parameters: 11823618\n",
            "Total number of conductivity images: 27048\n",
            "total training data shape: (23000, 1, 6, 41, 81)\n",
            "[Epoch 1/50] [Batch 359/360] [D loss: 0.830619] [G loss: 0.385165 /G_A loss: 0.640725/ G_C loss: 0.378758]\n",
            "[Epoch 2/50] [Batch 359/360] [D loss: 0.662937] [G loss: 0.322533 /G_A loss: 0.855457/ G_C loss: 0.313978]\n",
            "[Epoch 3/50] [Batch 359/360] [D loss: 0.689949] [G loss: 0.302666 /G_A loss: 0.645077/ G_C loss: 0.296215]\n",
            "[Epoch 4/50] [Batch 359/360] [D loss: 0.608766] [G loss: 0.336508 /G_A loss: 1.130831/ G_C loss: 0.325200]\n",
            "[Epoch 5/50] [Batch 359/360] [D loss: 0.629861] [G loss: 0.295878 /G_A loss: 0.819538/ G_C loss: 0.287683]\n",
            "[Epoch 6/50] [Batch 359/360] [D loss: 0.626531] [G loss: 0.278019 /G_A loss: 0.826556/ G_C loss: 0.269753]\n",
            "[Epoch 7/50] [Batch 359/360] [D loss: 0.700867] [G loss: 0.288308 /G_A loss: 0.929063/ G_C loss: 0.279017]\n",
            "[Epoch 8/50] [Batch 359/360] [D loss: 0.549117] [G loss: 0.275511 /G_A loss: 0.944741/ G_C loss: 0.266063]\n",
            "[Epoch 9/50] [Batch 359/360] [D loss: 0.635833] [G loss: 0.282239 /G_A loss: 0.984911/ G_C loss: 0.272390]\n",
            "epoch 10, done printing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 11, done printing\n",
            "[Epoch 10/50] [Batch 359/360] [D loss: 0.689653] [G loss: 0.272549 /G_A loss: 0.661343/ G_C loss: 0.265935]\n",
            "[Epoch 11/50] [Batch 359/360] [D loss: 0.639167] [G loss: 0.281721 /G_A loss: 1.173022/ G_C loss: 0.269991]\n",
            "[Epoch 12/50] [Batch 359/360] [D loss: 0.555962] [G loss: 0.270431 /G_A loss: 1.192755/ G_C loss: 0.258504]\n",
            "[Epoch 13/50] [Batch 359/360] [D loss: 0.562495] [G loss: 0.296917 /G_A loss: 0.948231/ G_C loss: 0.287435]\n",
            "[Epoch 14/50] [Batch 359/360] [D loss: 0.620493] [G loss: 0.267063 /G_A loss: 0.898006/ G_C loss: 0.258082]\n",
            "[Epoch 15/50] [Batch 359/360] [D loss: 0.481739] [G loss: 0.266879 /G_A loss: 1.107682/ G_C loss: 0.255803]\n",
            "[Epoch 16/50] [Batch 359/360] [D loss: 0.475665] [G loss: 0.315295 /G_A loss: 1.265028/ G_C loss: 0.302645]\n",
            "[Epoch 17/50] [Batch 359/360] [D loss: 0.591780] [G loss: 0.260418 /G_A loss: 0.836854/ G_C loss: 0.252050]\n",
            "[Epoch 18/50] [Batch 359/360] [D loss: 0.591803] [G loss: 0.271315 /G_A loss: 1.138041/ G_C loss: 0.259935]\n",
            "[Epoch 19/50] [Batch 359/360] [D loss: 0.474674] [G loss: 0.280590 /G_A loss: 1.236035/ G_C loss: 0.268230]\n",
            "epoch 20, done printing\n",
            "epoch 21, done printing\n",
            "[Epoch 20/50] [Batch 359/360] [D loss: 0.742195] [G loss: 0.264587 /G_A loss: 0.792610/ G_C loss: 0.256661]\n",
            "[Epoch 21/50] [Batch 359/360] [D loss: 0.560062] [G loss: 0.251706 /G_A loss: 0.888538/ G_C loss: 0.242820]\n",
            "[Epoch 22/50] [Batch 359/360] [D loss: 0.633219] [G loss: 0.262229 /G_A loss: 1.104974/ G_C loss: 0.251180]\n",
            "[Epoch 23/50] [Batch 359/360] [D loss: 0.520173] [G loss: 0.264999 /G_A loss: 1.131669/ G_C loss: 0.253683]\n",
            "[Epoch 24/50] [Batch 359/360] [D loss: 0.604448] [G loss: 0.260406 /G_A loss: 0.983967/ G_C loss: 0.250567]\n",
            "[Epoch 25/50] [Batch 359/360] [D loss: 0.585452] [G loss: 0.251579 /G_A loss: 1.186032/ G_C loss: 0.239719]\n",
            "[Epoch 26/50] [Batch 359/360] [D loss: 0.552671] [G loss: 0.254676 /G_A loss: 0.909178/ G_C loss: 0.245585]\n",
            "[Epoch 27/50] [Batch 359/360] [D loss: 0.604102] [G loss: 0.253716 /G_A loss: 0.985222/ G_C loss: 0.243863]\n",
            "[Epoch 28/50] [Batch 359/360] [D loss: 0.574588] [G loss: 0.248669 /G_A loss: 0.919679/ G_C loss: 0.239472]\n",
            "[Epoch 29/50] [Batch 359/360] [D loss: 0.690255] [G loss: 0.250139 /G_A loss: 0.748508/ G_C loss: 0.242653]\n",
            "epoch 30, done printing\n",
            "epoch 31, done printing\n",
            "[Epoch 30/50] [Batch 359/360] [D loss: 0.573268] [G loss: 0.257143 /G_A loss: 0.989842/ G_C loss: 0.247244]\n",
            "[Epoch 31/50] [Batch 359/360] [D loss: 0.601428] [G loss: 0.247180 /G_A loss: 0.917008/ G_C loss: 0.238010]\n",
            "[Epoch 32/50] [Batch 359/360] [D loss: 0.641675] [G loss: 0.244881 /G_A loss: 0.742245/ G_C loss: 0.237459]\n",
            "[Epoch 33/50] [Batch 359/360] [D loss: 0.735835] [G loss: 0.264755 /G_A loss: 1.065850/ G_C loss: 0.254097]\n",
            "[Epoch 34/50] [Batch 359/360] [D loss: 0.627769] [G loss: 0.247898 /G_A loss: 1.188438/ G_C loss: 0.236014]\n",
            "[Epoch 35/50] [Batch 359/360] [D loss: 0.617684] [G loss: 0.244533 /G_A loss: 0.766503/ G_C loss: 0.236868]\n",
            "[Epoch 36/50] [Batch 359/360] [D loss: 0.549402] [G loss: 0.249416 /G_A loss: 1.365433/ G_C loss: 0.235762]\n",
            "[Epoch 37/50] [Batch 359/360] [D loss: 0.512848] [G loss: 0.248178 /G_A loss: 1.168259/ G_C loss: 0.236495]\n",
            "[Epoch 38/50] [Batch 359/360] [D loss: 0.544558] [G loss: 0.244629 /G_A loss: 0.932318/ G_C loss: 0.235306]\n",
            "[Epoch 39/50] [Batch 359/360] [D loss: 0.550992] [G loss: 0.252693 /G_A loss: 0.904194/ G_C loss: 0.243651]\n",
            "epoch 40, done printing\n",
            "epoch 41, done printing\n",
            "[Epoch 40/50] [Batch 359/360] [D loss: 0.626043] [G loss: 0.253335 /G_A loss: 1.023682/ G_C loss: 0.243098]\n",
            "[Epoch 41/50] [Batch 359/360] [D loss: 0.722192] [G loss: 0.244689 /G_A loss: 0.889410/ G_C loss: 0.235795]\n",
            "[Epoch 42/50] [Batch 359/360] [D loss: 0.549614] [G loss: 0.240021 /G_A loss: 0.934623/ G_C loss: 0.230675]\n",
            "[Epoch 43/50] [Batch 359/360] [D loss: 0.568623] [G loss: 0.289362 /G_A loss: 1.089389/ G_C loss: 0.278469]\n",
            "[Epoch 44/50] [Batch 359/360] [D loss: 0.643413] [G loss: 0.243348 /G_A loss: 0.955398/ G_C loss: 0.233794]\n",
            "[Epoch 45/50] [Batch 359/360] [D loss: 0.549157] [G loss: 0.241535 /G_A loss: 0.970556/ G_C loss: 0.231829]\n",
            "[Epoch 46/50] [Batch 359/360] [D loss: 0.398549] [G loss: 0.246281 /G_A loss: 1.355126/ G_C loss: 0.232729]\n",
            "[Epoch 47/50] [Batch 359/360] [D loss: 0.536897] [G loss: 0.248645 /G_A loss: 0.976472/ G_C loss: 0.238880]\n",
            "[Epoch 48/50] [Batch 359/360] [D loss: 0.492106] [G loss: 0.237017 /G_A loss: 1.023041/ G_C loss: 0.226787]\n",
            "[Epoch 49/50] [Batch 359/360] [D loss: 0.560397] [G loss: 0.253080 /G_A loss: 1.029784/ G_C loss: 0.242782]\n",
            "epoch 50, done printing\n",
            "epoch 51, done printing\n",
            "[Epoch 50/50] [Batch 359/360] [D loss: 0.553801] [G loss: 0.240144 /G_A loss: 1.167986/ G_C loss: 0.228464]\n",
            "time for training: 18678.232300281525\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}