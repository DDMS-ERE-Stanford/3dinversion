{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAAE.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO5N0jEwkvKkDc6JdxoS1iT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZitongZhou/react_inverse/blob/master/CAAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S8Bd4yZgsZR",
        "outputId": "5531db11-b5c2-43e5-b089-d7c241cd8e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roo9l7s3UYRa"
      },
      "source": [
        "# CAAE 3d models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t44ZwK0bYK3G"
      },
      "source": [
        "## model definition part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_cb8ucUWPv"
      },
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "\n",
        "    return mu + eps*std\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.BatchNorm3d(in_features)]\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "            layers += [nn.Conv3d(in_features, filters, 3, 1, 1, bias=True)]\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)#, DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, inchannels=1, outchannels=2, filters=48, num_res_blocks=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        # input size, inchannels x 6 x 32 x 64\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=2, padding=1)\n",
        "        # state size. filters x 3 x 16 x 32\n",
        "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        # state size. filters x 3 x 16 x 32\n",
        "        self.trans = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        # state size. filters x 2 x 8 x 16\n",
        "        self.mu = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False)\n",
        "        self.logvar = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # img: inchannels x 6 x 32 x 64\n",
        "        out1 = self.conv1(img)        # filters x 3 x 16 x 32\n",
        "        out2 = self.res_blocks(out1)   # filters x 3 x 16 x 32\n",
        "        out3 = self.trans(out2)        # filters x 2 x 8 x 16\n",
        "\n",
        "        mu, logvar = self.mu(out3), self.logvar(out3)\n",
        "        z = reparameterization(mu, logvar) # latent dimension: outchannels x 2 x 8 x 16\n",
        "        return z\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48, num_res_blocks=1,num_upsample=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # First layer. input size, inchannels x 2 x 8 x 16\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # state size. filters x 2 x 8 x 16\n",
        "        # Residual blocks\n",
        "        self.res_block1 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks+1)])\n",
        "        self.transup1 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "        self.res_block2 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        self.transup2 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv3d(filters, outchannels, kernel_size=3, stride=1, padding=(0,1,1)),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        # x: in_channels x 2 x 8 x 16\n",
        "        out1 = self.conv1(z)          # filters x 2 x 8 x 16\n",
        "        out2 = self.res_block1(out1)   # filters x 2 x 8 x 16\n",
        "        out = torch.add(out1, out2)   # filters x 2 x 8 x 16\n",
        "        out3 = self.transup1(out)      # filters x 4 x 16 x 32\n",
        "        out4 = self.res_block2(out3)   # filters x 4 x 16 x 32\n",
        "\n",
        "        img = self.transup2(out4)     # outchannels x 6 x 32 x 64\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params= 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (inchannels) x 2 x 8 x 16\n",
        "            nn.Conv3d(inchannels, filters, 3, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 4 x 8\n",
        "            nn.Conv3d(filters, filters, 3, 1, 1, bias=True),\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 4 x 8\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(filters * 4 * 8,128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(128, outchannels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output1 = self.fc1(output)\n",
        "        output2 = self.fc2(output1)\n",
        "        return output2\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_5E4jwXsJf"
      },
      "source": [
        "## data loading and plotting block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shu_Pl3HXptF"
      },
      "source": [
        "def load_data(hdf5_dir, opt):\n",
        "    with open('/content/drive/MyDrive/react_inverse/training_data/kds.pkl', 'rb') as file:\n",
        "      kds = pk.load(file)\n",
        "      \n",
        "    with h5py.File(hdf5_dir + \"train/input_lhs{}.hdf5\".format(opt.n_train), 'r') as f:\n",
        "        x_train = f['dataset'][()]\n",
        "\n",
        "    with h5py.File(hdf5_dir + \"test/input_lhs{}.hdf5\".format(opt.n_test), 'r') as f:\n",
        "        x_test = f['dataset'][()]\n",
        "\n",
        "    print(\"total training data shape: {}\".format(x_train.shape))\n",
        "\n",
        "    data = torch.utils.data.TensorDataset(torch.FloatTensor(x_train))\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=opt.batch_size,\n",
        "                                              shuffle=True, num_workers=int(2))\n",
        "\n",
        "    return data_loader, x_test\n",
        "\n",
        "\n",
        "def plot_pred(samples, epoch, idx, output_dir):\n",
        "    Ncol = 3\n",
        "    Nrow = samples.shape[0] // Ncol\n",
        "\n",
        "    fig, axes = plt.subplots(Nrow, Ncol, figsize=(Ncol*4, Nrow*2.1))\n",
        "    fs = 16 # font size\n",
        "    for j, ax in enumerate(fig.axes):\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j < samples.shape[0]:\n",
        "\n",
        "            cax = ax.imshow(samples[j], cmap='jet', origin='lower', vmin=0, vmax=5)\n",
        "            cbar = plt.colorbar(cax, ax=ax, fraction=0.025, pad=0.04,\n",
        "                            format=ticker.ScalarFormatter(useMathText=True))\n",
        "            cbar.formatter.set_powerlimits((0, 0))\n",
        "            cbar.ax.yaxis.set_offset_position('left')\n",
        "            cbar.update_ticks()\n",
        "            cbar.ax.tick_params(axis='both', which='both', length=0)\n",
        "            cbar.ax.yaxis.get_offset_text().set_fontsize(fs-3)\n",
        "            cbar.ax.tick_params(labelsize=fs-2)\n",
        "\n",
        "    plt.savefig(output_dir+'/epoch_{}_{}.png'.format(epoch,idx), bbox_inches='tight',dpi=600)\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(\"epoch {}, done printing\".format(epoch))\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFwSG0DgYHN3"
      },
      "source": [
        "## training part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhaCZPXUngE"
      },
      "source": [
        "os.makedirs(\"images_CAAE\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--current-dir\", type=str, default=\"/afs/crc.nd.edu/user/s/smo/CAAE3D/\", help=\"data directory\")\n",
        "parser.add_argument(\"--n-epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
        "parser.add_argument('--n-train', type=int, default=37600, help='number of training data')\n",
        "parser.add_argument('--n-test', type=int, default=3000, help='number of training data')\n",
        "parser.add_argument(\"--batch-size\", type=int, default=32, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--lw\", type=float, default=0.01, help=\"adversarial loss weight\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--sample-interval\", type=int, default=10, help=\"interval between image sampling\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "date = 'experiments/Dec_26_CAAE3D'\n",
        "exp_dir = opt.current_dir + date + \"/N{}_Bts{}_Eps{}_lr{}_lw{}\".\\\n",
        "    format(opt.n_train, opt.batch_size, opt.n_epochs, opt.lr, opt.lw)\n",
        "\n",
        "output_dir = exp_dir + \"/predictions\"\n",
        "model_dir = exp_dir\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# loss functions\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "pixelwise_loss = torch.nn.L1Loss()\n",
        "\n",
        "nf, d, h, w = 2, 2, 8, 16\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "encoder = Encoder(outchannels=nf)\n",
        "decoder = Decoder(inchannels=nf)\n",
        "discriminator = Discriminator(inchannels=nf)\n",
        "\n",
        "print(\"number of parameters: {}\".format(encoder._n_parameters()+decoder._n_parameters()+discriminator._n_parameters()))\n",
        "\n",
        "if cuda:\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    pixelwise_loss.cuda()\n",
        "\n",
        "hdf5_dir = \"/afs/crc.nd.edu/user/s/smo/multi-fidelity-CNN/raw2/3D/\"\n",
        "dataloader, x_test = load_data(hdf5_dir, opt)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(encoder.parameters(), decoder.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    n_samples = 1\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (n_samples, nf, d, h, w))))\n",
        "    gen_imgs = decoder(z)\n",
        "    samples = np.squeeze(gen_imgs.data.cpu().numpy())\n",
        "    plot_pred(samples,epoch,0,output_dir)\n",
        "\n",
        "    idx = np.random.choice(opt.n_test, 1, replace=False)\n",
        "    real_imgs = x_test[[idx]]\n",
        "    real_imgs = (torch.FloatTensor(real_imgs)).cuda()\n",
        "    encoded_imgs = encoder(real_imgs)\n",
        "    decoded_imgs = decoder(encoded_imgs)\n",
        "    samples_gen  = np.squeeze(decoded_imgs.data.cpu().numpy())\n",
        "    samples_real = np.squeeze(real_imgs.data.cpu().numpy())\n",
        "\n",
        "    samples = np.vstack((samples_real[:3],samples_gen[:3],samples_real[3:],samples_gen[3:]))\n",
        "    plot_pred(samples,epoch+1,idx,output_dir)\n",
        "\n",
        "\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "for epoch in range(1,opt.n_epochs+1):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for i, (imgs,) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.shape[0],1).fill_(1.0), requires_grad=False)\n",
        "        fake  = Variable(Tensor(imgs.shape[0],1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        encoded_imgs = encoder(real_imgs)\n",
        "        decoded_imgs = decoder(encoded_imgs)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss_a = adversarial_loss(discriminator(encoded_imgs), valid)\n",
        "        g_loss_c = pixelwise_loss(decoded_imgs, real_imgs)\n",
        "\n",
        "        g_loss = opt.lw * g_loss_a + g_loss_c\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as discriminator ground truth\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], nf, d, h, w))))\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(z), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(encoded_imgs.detach()), fake)\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    if (epoch) % opt.sample_interval == 0:\n",
        "        test(epoch)\n",
        "\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f /G_A loss: %f/ G_C loss: %f]\"\n",
        "        % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), g_loss_a.item(), g_loss_c.item())\n",
        "    )\n",
        "\n",
        "torch.save(decoder.state_dict(), model_dir + '/AAE_decoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(encoder.state_dict(), model_dir + '/AAE_encoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(discriminator.state_dict(), model_dir + '/AAE_discriminator_epoch{}.pth'.format(opt.n_epochs))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}