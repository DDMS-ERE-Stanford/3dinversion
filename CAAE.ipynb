{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAAE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ej_5E4jwXsJf"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOSnKSMzT2V+BvBYlk9o/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZitongZhou/react_inverse/blob/master/CAAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S8Bd4yZgsZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa1d2a9-75fa-468a-f3b0-3b9f8f1b7ca0"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import sys\n",
        "import pickle as pk\n",
        "import time\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, MultiStepLR\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.gridspec as gridspec\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roo9l7s3UYRa"
      },
      "source": [
        "# CAAE 3d models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t44ZwK0bYK3G"
      },
      "source": [
        "## model definition part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_cb8ucUWPv"
      },
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    eps = torch.randn_like(std)\n",
        "    #randn_like: Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.\n",
        "    #return: random gaussian sample from distribution with mu and exp(logvar/2)\n",
        "    return mu + eps*std\n",
        "\n",
        "\n",
        "class DenseResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    The core module of paper: (Residual Dense Network for Image Super-Resolution, CVPR 18)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(DenseResidualBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "\n",
        "        def block(in_features, non_linearity=True):\n",
        "            layers = [nn.BatchNorm3d(in_features)]\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "            layers += [nn.Conv3d(in_features, filters, 3, 1, 1, bias=True)] # does not change state size\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        self.b1 = block(in_features=1 * filters)\n",
        "        self.b2 = block(in_features=2 * filters)\n",
        "        self.b3 = block(in_features=3 * filters)\n",
        "        self.b4 = block(in_features=4 * filters)\n",
        "        self.b5 = block(in_features=5 * filters, non_linearity=False)\n",
        "        self.blocks = [self.b1, self.b2, self.b3, self.b4, self.b5]\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = x\n",
        "        for block in self.blocks:\n",
        "            out = block(inputs)\n",
        "            inputs = torch.cat([inputs, out], 1)\n",
        "        return out.mul(self.res_scale) + x\n",
        "\n",
        "\n",
        "class ResidualInResidualDenseBlock(nn.Module):\n",
        "    def __init__(self, filters, res_scale=0.2):\n",
        "        super(ResidualInResidualDenseBlock, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.dense_blocks = nn.Sequential(\n",
        "            DenseResidualBlock(filters), DenseResidualBlock(filters), DenseResidualBlock(filters)#, DenseResidualBlock(filters)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense_blocks(x).mul(self.res_scale) + x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, inchannels=1, outchannels=2, filters=48, num_res_blocks=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        # input size, inchannels x 6 x 41 x 81\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=2, padding=1)\n",
        "        # state size. filters x 3 x 21 x 41\n",
        "        self.res_blocks = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        # state size. filters x 3 x 21 x 41\n",
        "        self.trans = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        # state size. filters x 2 x 11 x 21\n",
        "        self.mu = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False) #does not change state size.\n",
        "        self.logvar = nn.Conv3d(filters, outchannels, 3, 1, 1, bias=False) #does not change state size.\n",
        "\n",
        "    def forward(self, img):\n",
        "        # img: inchannels x 6 x 41 x 81\n",
        "        out1 = self.conv1(img)        # filters x 3 x 21 x 41\n",
        "        out2 = self.res_blocks(out1)   # filters x 3 x 21 x 41\n",
        "        out3 = self.trans(out2)        # filters x 2 x 11 x 21\n",
        "\n",
        "        mu, logvar = self.mu(out3), self.logvar(out3)\n",
        "        z = reparameterization(mu, logvar) # latent dimension: outchannels x 2 x 11 x 21\n",
        "        return z\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48, num_res_blocks=1,num_upsample=2):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # First layer. input size, inchannels x 2 x 8 x 16\n",
        "        self.conv1 = nn.Conv3d(inchannels, filters, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # state size. filters x 2 x 8 x 16\n",
        "        # Residual blocks\n",
        "        self.res_block1 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks+1)])\n",
        "        self.transup1 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(4, 21, 41), mode='nearest'),\n",
        "            nn.Conv3d(filters, filters, kernel_size=3, stride=1, padding=1), #does not change state size\n",
        "        )\n",
        "        self.res_block2 = nn.Sequential(*[ResidualInResidualDenseBlock(filters) for _ in range(num_res_blocks)])\n",
        "        self.transup2 = nn.Sequential(\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Upsample(size=(8, 41, 81), mode='nearest'),\n",
        "            nn.Conv3d(filters, outchannels, kernel_size=3, stride=1, padding=(0,1,1)), # reduce the first dimension by 2\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        # x: in_channels x 2 x 8 x 16\n",
        "        out1 = self.conv1(z)          # filters x 2 x 8 x 16\n",
        "        out2 = self.res_block1(out1)   # filters x 2 x 8 x 16\n",
        "        out = torch.add(out1, out2)   # filters x 2 x 8 x 16\n",
        "        out3 = self.transup1(out)      # filters x 4 x 16 x 32\n",
        "        out4 = self.res_block2(out3)   # filters x 4 x 16 x 32\n",
        "\n",
        "        img = self.transup2(out4)     # outchannels x 6 x 32 x 64\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params= 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, inchannels=2, outchannels=1, filters=48):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # input is (inchannels) x 2 x 11 x 21\n",
        "            nn.Conv3d(inchannels, filters, 3, 2, 1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 6 x 11\n",
        "            nn.Conv3d(filters, filters, 3, 1, 1, bias=True),\n",
        "            nn.BatchNorm3d(filters),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # state size. (filters) x 1 x 6 x 11\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(filters * 6 * 11,128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(128, outchannels),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output1 = self.fc1(output)\n",
        "        output2 = self.fc2(output1)\n",
        "        return output2\n",
        "\n",
        "    def _n_parameters(self):\n",
        "        n_params = 0\n",
        "        for name, param in self.named_parameters():\n",
        "            n_params += param.numel()\n",
        "        return n_params"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_5E4jwXsJf"
      },
      "source": [
        "## data loading and plotting block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shu_Pl3HXptF"
      },
      "source": [
        "def load_data(opt):\n",
        "    with open('/content/drive/MyDrive/react_inverse/training_data/kds.pkl', 'rb') as file:\n",
        "      kds = np.expand_dims(np.asarray(pk.load(file)), axis=1)\n",
        "    print('Total number of conductivity images:', len(kds))\n",
        "\n",
        "    x_train = kds[:opt.n_train]\n",
        "    x_test = kds[opt.n_train: opt.n_train + opt.n_test]\n",
        "\n",
        "    print(\"total training data shape: {}\".format(x_train.shape))\n",
        "\n",
        "    data = torch.utils.data.TensorDataset(torch.FloatTensor(x_train))\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=opt.batch_size,\n",
        "                                              shuffle=True, num_workers=int(2))\n",
        "    return data_loader, x_test\n",
        "\n",
        "def simple_plot(c_map, title=''):\n",
        "    nx = 81\n",
        "    ny = 41\n",
        "    Lx = 2500\n",
        "    Ly = 1250\n",
        "\n",
        "    x = np.linspace(0, Lx, nx)\n",
        "    y = np.linspace(0, Ly, ny)\n",
        "    X,Y = np.meshgrid(x, y)\n",
        "    fig, axs = plt.subplots(1,1)\n",
        "#        axs.set_xlabel('x(m)')\n",
        "#        axs.set_ylabel('y(m)')\n",
        "    axs.set_xlim(0,Lx)\n",
        "    axs.set_ylim(0,Ly)\n",
        "    c01map = axs.imshow(c_map, cmap='jet',\n",
        "              extent=[x.min(), x.max(), y.min(), y.max()],\n",
        "              vmin=c_map.min(), vmax = c_map.max(),\n",
        "              origin='lower')\n",
        "    fig.colorbar(c01map, ax=axs,shrink=0.62)\n",
        "    name = title + '.pdf'\n",
        "    plt.title(title)\n",
        "#         fig.savefig('images/'+name, format='pdf',bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "def plot_pred(samples, epoch, idx, output_dir):\n",
        "    Ncol = 3\n",
        "    Nrow = samples.shape[0] // Ncol\n",
        "\n",
        "    fig, axes = plt.subplots(Nrow, Ncol, figsize=(Ncol*4, Nrow*2.1))\n",
        "    fs = 16 # font size\n",
        "    for j, ax in enumerate(fig.axes):\n",
        "        ax.set_aspect('equal')\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        if j < samples.shape[0]:\n",
        "\n",
        "            cax = ax.imshow(samples[j], cmap='jet', origin='lower', vmin=0, vmax=5)\n",
        "            cbar = plt.colorbar(cax, ax=ax, fraction=0.025, pad=0.04,\n",
        "                            format=ticker.ScalarFormatter(useMathText=True))\n",
        "            cbar.formatter.set_powerlimits((0, 0))\n",
        "            cbar.ax.yaxis.set_offset_position('left')\n",
        "            cbar.update_ticks()\n",
        "            cbar.ax.tick_params(axis='both', which='both', length=0)\n",
        "            cbar.ax.yaxis.get_offset_text().set_fontsize(fs-3)\n",
        "            cbar.ax.tick_params(labelsize=fs-2)\n",
        "\n",
        "    plt.savefig(output_dir+'/epoch_{}_{}.png'.format(epoch,idx), bbox_inches='tight',dpi=600)\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(\"epoch {}, done printing\".format(epoch))\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFwSG0DgYHN3"
      },
      "source": [
        "## training part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzhaCZPXUngE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861d8e06-b7b9-4d39-bd56-c4f2a0549ad1"
      },
      "source": [
        "os.makedirs(\"images_CAAE\", exist_ok=True)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-f')\n",
        "parser.add_argument(\"--current-dir\", type=str, default=\"/content/drive/MyDrive/react_inverse/CAAE/\", help=\"data directory\")\n",
        "parser.add_argument(\"--n-epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
        "parser.add_argument('--n-train', type=int, default=23000, help='number of training data')\n",
        "parser.add_argument('--n-test', type=int, default=4000, help='number of training data')\n",
        "parser.add_argument(\"--batch-size\", type=int, default=64, help=\"size of the batches\")\n",
        "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
        "parser.add_argument(\"--lw\", type=float, default=0.01, help=\"adversarial loss weight\")\n",
        "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
        "parser.add_argument(\"--sample-interval\", type=int, default=10, help=\"interval between image sampling\")\n",
        "opt = parser.parse_args()\n",
        "print(opt)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "date = 'experiments/Feb_14_CAAE3D'\n",
        "exp_dir = opt.current_dir + date + \"/N{}_Bts{}_Eps{}_lr{}_lw{}\".\\\n",
        "    format(opt.n_train, opt.batch_size, opt.n_epochs, opt.lr, opt.lw)\n",
        "\n",
        "output_dir = exp_dir + \"/predictions\"\n",
        "model_dir = exp_dir\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "# loss functions\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "pixelwise_loss = torch.nn.L1Loss()\n",
        "\n",
        "nf, d, h, w = 2, 2, 11, 21\n",
        "\n",
        "# Initialize generator and discriminator\n",
        "encoder = Encoder(outchannels=nf)\n",
        "decoder = Decoder(inchannels=nf)\n",
        "discriminator = Discriminator(inchannels=nf)\n",
        "\n",
        "print(\"number of parameters: {}\".format(encoder._n_parameters()+decoder._n_parameters()+discriminator._n_parameters()))\n",
        "\n",
        "if cuda:\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()\n",
        "    discriminator.cuda()\n",
        "    adversarial_loss.cuda()\n",
        "    pixelwise_loss.cuda()\n",
        "\n",
        "dataloader, x_test = load_data(opt)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(\n",
        "    itertools.chain(encoder.parameters(), decoder.parameters()), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    n_samples = 1\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (n_samples, nf, d, h, w))))\n",
        "    gen_imgs = decoder(z)\n",
        "    samples = np.squeeze(gen_imgs.data.cpu().numpy())\n",
        "    plot_pred(samples,epoch,0,output_dir)\n",
        "\n",
        "    idx = np.random.choice(opt.n_test, 1, replace=False)\n",
        "    real_imgs = x_test[[idx]]\n",
        "    real_imgs = (torch.FloatTensor(real_imgs)).cuda()\n",
        "    encoded_imgs = encoder(real_imgs)\n",
        "    decoded_imgs = decoder(encoded_imgs)\n",
        "    samples_gen  = np.squeeze(decoded_imgs.data.cpu().numpy())\n",
        "    samples_real = np.squeeze(real_imgs.data.cpu().numpy())\n",
        "\n",
        "    samples = np.vstack((samples_real[:3],samples_gen[:3],samples_real[3:],samples_gen[3:]))\n",
        "    plot_pred(samples,epoch+1,idx,output_dir)\n",
        "\n",
        "start = time.time()\n",
        "# ----------\n",
        "#  Training\n",
        "# ----------\n",
        "for epoch in range(1,opt.n_epochs+1):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for i, (imgs,) in enumerate(dataloader):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = Variable(Tensor(imgs.shape[0],1).fill_(1.0), requires_grad=False)\n",
        "        fake  = Variable(Tensor(imgs.shape[0],1).fill_(0.0), requires_grad=False)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = Variable(imgs.type(Tensor))\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        encoded_imgs = encoder(real_imgs)\n",
        "        decoded_imgs = decoder(encoded_imgs)\n",
        "\n",
        "        # Loss measures generator's ability to fool the discriminator\n",
        "        g_loss_a = adversarial_loss(discriminator(encoded_imgs), valid)\n",
        "        g_loss_c = pixelwise_loss(decoded_imgs, real_imgs)\n",
        "\n",
        "        g_loss = opt.lw * g_loss_a + g_loss_c\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Sample noise as discriminator ground truth\n",
        "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], nf, d, h, w))))\n",
        "\n",
        "        # Measure discriminator's ability to classify real from generated samples\n",
        "        real_loss = adversarial_loss(discriminator(z), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(encoded_imgs.detach()), fake)\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    if (epoch) % opt.sample_interval == 0:\n",
        "        test(epoch)\n",
        "\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f /G_A loss: %f/ G_C loss: %f]\"\n",
        "        % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), g_loss_a.item(), g_loss_c.item())\n",
        "    )\n",
        "\n",
        "torch.save(decoder.state_dict(), model_dir + '/AAE_decoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(encoder.state_dict(), model_dir + '/AAE_encoder_epoch{}.pth'.format(opt.n_epochs))\n",
        "torch.save(discriminator.state_dict(), model_dir + '/AAE_discriminator_epoch{}.pth'.format(opt.n_epochs))\n",
        "print('time for training:', time.time()-start)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(b1=0.5, b2=0.999, batch_size=64, current_dir='/content/drive/MyDrive/react_inverse/CAAE/', f='/root/.local/share/jupyter/runtime/kernel-672d7039-7b10-4a8a-863d-3bc4d652fec0.json', lr=0.0002, lw=0.01, n_epochs=5, n_test=4000, n_train=23000, sample_interval=10)\n",
            "number of parameters: 11823618\n",
            "Total number of conductivity images: 27048\n",
            "total training data shape: (23000, 1, 6, 41, 81)\n",
            "[Epoch 1/5] [Batch 359/360] [D loss: 0.537843] [G loss: 0.397050 /G_A loss: 0.904018/ G_C loss: 0.388009]\n",
            "[Epoch 2/5] [Batch 359/360] [D loss: 0.711070] [G loss: 0.339704 /G_A loss: 0.704244/ G_C loss: 0.332661]\n",
            "[Epoch 3/5] [Batch 359/360] [D loss: 0.639026] [G loss: 0.305158 /G_A loss: 0.826207/ G_C loss: 0.296896]\n",
            "[Epoch 4/5] [Batch 359/360] [D loss: 0.659485] [G loss: 0.312934 /G_A loss: 0.955066/ G_C loss: 0.303383]\n",
            "[Epoch 5/5] [Batch 359/360] [D loss: 0.668907] [G loss: 0.315022 /G_A loss: 0.885027/ G_C loss: 0.306172]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2aY8mbzbrzu",
        "outputId": "7ab3528d-7b7c-40c7-ba3f-090576c8fb4d"
      },
      "source": [
        "test(epoch)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 5, done printing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:71: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 6, done printing\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}